{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you use Kaggle's P100 GPU. This notebook has not been tested with any other GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T05:44:40.783010Z",
     "iopub.status.busy": "2025-05-06T05:44:40.782776Z",
     "iopub.status.idle": "2025-05-06T05:44:50.221788Z",
     "shell.execute_reply": "2025-05-06T05:44:50.221072Z",
     "shell.execute_reply.started": "2025-05-06T05:44:40.782992Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33munoutsch\u001b[0m (\u001b[33mclickb8-anlp-team\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config done...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from typing import Literal\n",
    "\n",
    "TRAIN_TYPE: Literal[\"SFT\", \"DPO\"] = \"DPO\"  # SFT or DPO\n",
    "\n",
    "MAX_STEPS = 1  # edit this after debugging\n",
    "LIMIT = 1  # edit this after debugging\n",
    "\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "    os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
    "    wandb.login(key=wandb_api_key)\n",
    "except Exception as e:\n",
    "    print(\"WANDB_API_KEY not set or failed to load.\")\n",
    "    print(\"Reason:\", str(e))\n",
    "    print(\"In Kaggle, add it via Add-ons → Secrets → Add Secret.\")\n",
    "\n",
    "MODEL_BASE_UIDS = [\n",
    "    \"unsloth/llama-3.2-1B-bnb-4bit\",\n",
    "    \"unsloth/llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
    "]\n",
    "# those are our finetuned SFT models\n",
    "MODEL_SFT_UIDS = [\"mzarev/Llama-3.1-8B-bnb-4bit-mix-500st\"]\n",
    "DATASET_SFT_UIDS = [\n",
    "    \"allenai/tulu-3-sft-personas-math-grade\",\n",
    "    \"allenai/tulu-3-sft-personas-math\",\n",
    "    \"allenai/tulu-3-sft-personas-instruction-following\",\n",
    "    \"allenai/tulu-3-sft-personas-algebra\",\n",
    "    \"allenai/tulu-3-sft-personas-code\",\n",
    "]\n",
    "DATASET_SFT_MIXTURE_UIDS = [\"allenai/tulu-3-sft-mixture\"]\n",
    "DATASET_DPO_UIDS = [\n",
    "    \"allenai/llama-3.1-tulu-3-8b-preference-mixture\",\n",
    "    \"allenai/llama-3.1-tulu-3-70b-preference-mixture\",\n",
    "    \"allenai/llama-3.1-tulu-3-405b-preference-mixture\",\n",
    "]\n",
    "LM_EVAL_UIDS = [\n",
    "    \"hellaswag\",\n",
    "    \"gsm8k\",\n",
    "    \"arc_easy\",\n",
    "    \"truthfulqa\",\n",
    "    \"winogrande\",\n",
    "    \"humaneval\",\n",
    "]\n",
    "\n",
    "\n",
    "MODEL_BASE_UID = MODEL_BASE_UIDS[2]  # choose depending on choice of SFT model (if any)\n",
    "MODEL_FINETUNED_UID = MODEL_SFT_UIDS[0]  # choose\n",
    "DATASET_UIDS = DATASET_DPO_UIDS  # make sure this represents the datasets you're currently interested in\n",
    "DATASET_UID = DATASET_UIDS[0]  # choose your dataset\n",
    "\n",
    "\n",
    "print(\"Config done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\n",
    "!pip install -e ./lm-evaluation-harness/.\n",
    "!pip install unsloth transformers datasets wandb pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import wandb\n",
    "from transformers import BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig, DPOTrainer, DPOConfig\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "model_name = MODEL_BASE_UID.split(\"/\")[-1]\n",
    "dataset_name = DATASET_UID.split(\"/\")[-1]\n",
    "\n",
    "if TRAIN_TYPE == \"SFT\":\n",
    "    base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=MODEL_BASE_UID, max_seq_length=2048, dtype=None, load_in_4bit=True\n",
    "    )\n",
    "    base_model.save_pretrained(f\"{model_name}\")\n",
    "\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        base_model,\n",
    "        r=16,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "        bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
    "        use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "        max_seq_length=2048,\n",
    "        use_rslora=False,  # We support rank stabilized LoRA\n",
    "        loftq_config=None,  # And LoftQ\n",
    "    )\n",
    "else:\n",
    "    # this is our LoRA-adapted SFT model\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=MODEL_FINETUNED_UID,\n",
    "        max_seq_length=2048,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    # this is the original base model\n",
    "    ref_model, _ = FastLanguageModel.from_pretrained(\n",
    "        model_name=MODEL_BASE_UID,\n",
    "        max_seq_length=2048,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "\n",
    "    # don't want to train the reference model\n",
    "    for param in ref_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.config.quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "train_dataset = load_dataset(DATASET_UID, split=\"train\")\n",
    "print(train_dataset[0].keys())\n",
    "print(json.dumps(train_dataset[0][\"messages\"], indent=2))\n",
    "\n",
    "\n",
    "if TRAIN_TYPE == \"SFT\":\n",
    "\n",
    "    def formatting_func(examples):\n",
    "        messages = examples[\"messages\"]\n",
    "        texts = [\n",
    "            \"\".join([m[\"content\"].strip() + \"\\n\" for m in convo]).strip()\n",
    "            for convo in messages\n",
    "        ]\n",
    "        return {\"text\": texts}\n",
    "\n",
    "    train_dataset = train_dataset.map(formatting_func, batched=True)\n",
    "\n",
    "wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
    "wandb.init(\n",
    "    project=\"pm-pt\",\n",
    "    name=f\"{model_name}_{dataset_name}\",\n",
    "    config={\n",
    "        \"model\": MODEL_BASE_UID,\n",
    "        \"dataset\": DATASET_UID,\n",
    "        \"max_steps\": MAX_STEPS,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"batch_size\": 2,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "    },\n",
    ")\n",
    "\n",
    "if TRAIN_TYPE == \"SFT\":\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        args=SFTConfig(\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=2048,\n",
    "            learning_rate=2e-4,\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=5,\n",
    "            max_steps=MAX_STEPS,\n",
    "            report_to=\"wandb\",\n",
    "            run_name=f\"{model_name}_{dataset_name}\",\n",
    "            output_dir=\"outputs\",\n",
    "            optim=\"adamw_8bit\",\n",
    "        ),\n",
    "    )\n",
    "else:\n",
    "    trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        ref_model=ref_model,\n",
    "        args=DPOConfig(\n",
    "            beta=0.1,\n",
    "            max_length=2048,\n",
    "            learning_rate=2e-4,\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=5,\n",
    "            max_steps=MAX_STEPS,\n",
    "            report_to=\"wandb\",\n",
    "            run_name=f\"{model_name}_{dataset_name}\",\n",
    "            output_dir=\"outputs\",\n",
    "            optim=\"adamw_8bit\",\n",
    "        ),\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "model.save_pretrained(f\"{model_name}_{TRAIN_TYPE}_finetuned_{dataset_name}\")\n",
    "tokenizer.save_pretrained(f\"{model_name}_{TRAIN_TYPE}_finetuned_{dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "model_name = MODEL_BASE_UID.split(\"/\")[-1]\n",
    "dataset_name = DATASET_UID.split(\"/\")[-1]\n",
    "\n",
    "peft_path = f\"./{model_name}_{TRAIN_TYPE} _finetuned_{dataset_name}\"\n",
    "\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
    "\n",
    "tasks_str = \",\".join(LM_EVAL_UIDS)\n",
    "\n",
    "command = [\n",
    "    \"lm_eval\",\n",
    "    \"--model\",\n",
    "    \"hf\",\n",
    "    \"--model_args\",\n",
    "    f\"pretrained=./{model_name},peft={peft_path}\",\n",
    "    \"--tasks\",\n",
    "    tasks_str,\n",
    "    \"--confirm_run_unsafe_code\",\n",
    "    \"--device\",\n",
    "    \"cuda\",\n",
    "    \"--batch_size\",\n",
    "    \"auto\",\n",
    "    \"--limit\",\n",
    "    str(LIMIT),\n",
    "]\n",
    "\n",
    "subprocess.run(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Excel Sheet Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T10:30:28.150917Z",
     "iopub.status.busy": "2025-04-30T10:30:28.150266Z",
     "iopub.status.idle": "2025-04-30T10:30:28.200472Z",
     "shell.execute_reply": "2025-04-30T10:30:28.199623Z",
     "shell.execute_reply.started": "2025-04-30T10:30:28.150892Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created empty eval_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "columns = [\"model_uid\", \"dataset_uid\"] + LM_EVAL_UIDS\n",
    "\n",
    "model_dataset_pairs = list(product(MODEL_BASE_UIDS, DATASET_UIDS))\n",
    "\n",
    "empty_eval_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for model_uid, dataset_uid in model_dataset_pairs:\n",
    "    row = {\n",
    "        \"model_uid\": model_uid,\n",
    "        \"dataset_uid\": dataset_uid,\n",
    "    }\n",
    "    for task in LM_EVAL_UIDS:\n",
    "        row[task] = None\n",
    "    empty_eval_df.loc[len(empty_eval_df)] = row\n",
    "\n",
    "empty_eval_df.to_excel(\"empty_eval_results.xlsx\", index=False)\n",
    "\n",
    "print(\"Created empty eval_results.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "pm-pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
