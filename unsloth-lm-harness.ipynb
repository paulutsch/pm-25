{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to use Kaggle's T4\\*2 GPUs for training. P100 GPUs may encounter issues with some torch-dependent libraries due to their age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T19:32:32.993652Z",
     "iopub.status.busy": "2025-05-06T19:32:32.992976Z",
     "iopub.status.idle": "2025-05-06T19:32:33.226596Z",
     "shell.execute_reply": "2025-05-06T19:32:33.225914Z",
     "shell.execute_reply.started": "2025-05-06T19:32:32.993621Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from typing import Literal\n",
    "\n",
    "TRAIN_TYPE: Literal[\"SFT\", \"DPO\"] = \"DPO\"  # SFT or DPO\n",
    "\n",
    "USE_ACCELERATE = True # set to True if you have multiple GPUs, it fastens the evaluation time\n",
    "MAX_STEPS = 1  # set to 500 after debugging\n",
    "LIMIT = 10  # set to None after debugging\n",
    "\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "    os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
    "    wandb.login(key=wandb_api_key)\n",
    "except Exception as e:\n",
    "    print(\"WANDB_API_KEY not set or failed to load.\")\n",
    "    print(\"Reason:\", str(e))\n",
    "    print(\"In Kaggle, add it via Add-ons → Secrets → Add Secret.\")\n",
    "\n",
    "MODEL_BASE_UIDS = [\n",
    "    \"unsloth/llama-3.2-1B-bnb-4bit\",\n",
    "    \"unsloth/llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
    "]\n",
    "# those are our finetuned SFT models\n",
    "MODEL_SFT_UIDS = [\n",
    "    \"alextsiak/llama-3.2-1B-bnb-4bit-mix-500st\",\n",
    "    \"rodionzorin/llama-3.2-3B-bnb-4bit_finetuned_tulu-3-sft-mixture\",\n",
    "    \"mzarev/Llama-3.1-8B-bnb-4bit-mix-500st\",\n",
    "]\n",
    "DATASET_SFT_UIDS = [\n",
    "    \"allenai/tulu-3-sft-personas-math-grade\",\n",
    "    \"allenai/tulu-3-sft-personas-math\",\n",
    "    \"allenai/tulu-3-sft-personas-instruction-following\",\n",
    "    \"allenai/tulu-3-sft-personas-algebra\",\n",
    "    \"allenai/tulu-3-sft-personas-code\",\n",
    "]\n",
    "DATASET_SFT_MIXTURE_UIDS = [\"allenai/tulu-3-sft-mixture\"]\n",
    "DATASET_DPO_UIDS = [\n",
    "    \"allenai/llama-3.1-tulu-3-8b-preference-mixture\",\n",
    "    \"allenai/llama-3.1-tulu-3-70b-preference-mixture\",\n",
    "    \"allenai/llama-3.1-tulu-3-405b-preference-mixture\",\n",
    "]\n",
    "LM_EVAL_UIDS = [\n",
    "    \"hellaswag\",\n",
    "    \"gsm8k\",\n",
    "    \"arc_easy\",\n",
    "    \"truthfulqa\",\n",
    "    \"winogrande\",\n",
    "    \"humaneval\",\n",
    "]\n",
    "\n",
    "\n",
    "MODEL_BASE_UID = MODEL_BASE_UIDS[\n",
    "    1\n",
    "]  # choose depending on choice of finetuned model (if any)\n",
    "MODEL_FINETUNED_UID = MODEL_SFT_UIDS[1]  # choose\n",
    "DATASET_UIDS = DATASET_DPO_UIDS  # make sure this represents the datasets you're currently interested in\n",
    "DATASET_UID = DATASET_UIDS[2]  # choose your dataset\n",
    "\n",
    "\n",
    "print(\"Config done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-06T19:05:59.855265Z",
     "iopub.status.busy": "2025-05-06T19:05:59.854888Z",
     "iopub.status.idle": "2025-05-06T19:10:50.759825Z",
     "shell.execute_reply": "2025-05-06T19:10:50.758639Z",
     "shell.execute_reply.started": "2025-05-06T19:05:59.855246Z"
    }
   },
   "outputs": [],
   "source": [
    "!git clone -q https://github.com/EleutherAI/lm-evaluation-harness.git\n",
    "!pip install -q -e ./lm-evaluation-harness/.\n",
    "!pip install -q unsloth transformers datasets wandb pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T19:32:42.158525Z",
     "iopub.status.busy": "2025-05-06T19:32:42.157708Z",
     "iopub.status.idle": "2025-05-06T19:33:20.952389Z",
     "shell.execute_reply": "2025-05-06T19:33:20.951752Z",
     "shell.execute_reply.started": "2025-05-06T19:32:42.158492Z"
    }
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import wandb\n",
    "from transformers import BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig, DPOTrainer, DPOConfig\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_name = MODEL_BASE_UID.split(\"/\")[-1]\n",
    "dataset_name = DATASET_UID.split(\"/\")[-1]\n",
    "\n",
    "if TRAIN_TYPE == \"SFT\":\n",
    "    base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=MODEL_BASE_UID, max_seq_length=2048, dtype=None, load_in_4bit=True\n",
    "    )\n",
    "    base_model.save_pretrained(model_name)\n",
    "\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        base_model,\n",
    "        r=16,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "        bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
    "        use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "        max_seq_length=2048,\n",
    "        use_rslora=False,  # We support rank stabilized LoRA\n",
    "        loftq_config=None,  # And LoftQ\n",
    "    )\n",
    "else:\n",
    "    # this is the original base model\n",
    "    ref_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=MODEL_BASE_UID,\n",
    "        max_seq_length=2048,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    ref_model.save_pretrained(model_name)\n",
    "    tokenizer.save_pretrained(model_name)\n",
    "    \n",
    "    # this is our LoRA-adapted SFT model\n",
    "    model, _ = FastLanguageModel.from_pretrained(\n",
    "        model_name=MODEL_FINETUNED_UID,\n",
    "        max_seq_length=2048,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "\n",
    "    tokenizer.chat_template = \"\"\"<s>[INST] {{ user }} [/INST] {{ assistant }}</s>\"\"\"\n",
    "\n",
    "    # don't want to train the reference model\n",
    "    for param in ref_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.config.quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "train_dataset = load_dataset(DATASET_UID, split=\"train\")\n",
    "print(train_dataset[0].keys())\n",
    "\n",
    "\n",
    "if TRAIN_TYPE == \"SFT\":\n",
    "    print(json.dumps(train_dataset[0][\"messages\"], indent=2))\n",
    "\n",
    "    def formatting_func(examples):\n",
    "        messages = examples[\"messages\"]\n",
    "        texts = [\n",
    "            \"\".join([m[\"content\"].strip() + \"\\n\" for m in convo]).strip()\n",
    "            for convo in messages\n",
    "        ]\n",
    "        return {\"text\": texts}\n",
    "\n",
    "    train_dataset = train_dataset.map(formatting_func, batched=True)\n",
    "\n",
    "wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
    "wandb.init(\n",
    "    project=\"pm-pt\",\n",
    "    name=f\"{model_name}_{dataset_name}\",\n",
    "    config={\n",
    "        \"model\": MODEL_BASE_UID,\n",
    "        \"dataset\": DATASET_UID,\n",
    "        \"max_steps\": MAX_STEPS,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"batch_size\": 2,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "    },\n",
    ")\n",
    "\n",
    "if TRAIN_TYPE == \"SFT\":\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        args=SFTConfig(\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=2048,\n",
    "            learning_rate=2e-4,\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=5,\n",
    "            max_steps=MAX_STEPS,\n",
    "            report_to=\"wandb\",\n",
    "            run_name=f\"{model_name}_{dataset_name}\",\n",
    "            output_dir=\"outputs\",\n",
    "            optim=\"adamw_8bit\",\n",
    "        ),\n",
    "    )\n",
    "else:\n",
    "    trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        ref_model=ref_model,\n",
    "        args=DPOConfig(\n",
    "            beta=0.1,\n",
    "            max_length=2048,\n",
    "            learning_rate=2e-4,\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=5,\n",
    "            max_steps=MAX_STEPS,\n",
    "            report_to=\"wandb\",\n",
    "            run_name=f\"{model_name}_{dataset_name}\",\n",
    "            output_dir=\"outputs\",\n",
    "            optim=\"adamw_8bit\",\n",
    "        ),\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "model.save_pretrained(f\"{model_name}_{TRAIN_TYPE}_finetuned_{dataset_name}\")\n",
    "tokenizer.save_pretrained(f\"{model_name}_{TRAIN_TYPE}_finetuned_{dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T19:33:25.903050Z",
     "iopub.status.busy": "2025-05-06T19:33:25.902687Z",
     "iopub.status.idle": "2025-05-06T19:35:26.959925Z",
     "shell.execute_reply": "2025-05-06T19:35:26.959036Z",
     "shell.execute_reply.started": "2025-05-06T19:33:25.903026Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "model_name = MODEL_BASE_UID.split(\"/\")[-1]\n",
    "dataset_name = DATASET_UID.split(\"/\")[-1]\n",
    "\n",
    "peft_path = f\"./{model_name}_{TRAIN_TYPE}_finetuned_{dataset_name}\"\n",
    "\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
    "\n",
    "tasks_str = \",\".join(LM_EVAL_UIDS)\n",
    "\n",
    "# Base arguments shared across both modes\n",
    "base_args = [\n",
    "    \"--model\", \"hf\",\n",
    "    \"--model_args\", f\"pretrained={model_name},peft={peft_path}\",\n",
    "    \"--tasks\", tasks_str,\n",
    "    \"--confirm_run_unsafe_code\",\n",
    "    \"--device\", \"cuda\",\n",
    "    \"--batch_size\", \"auto\",\n",
    "]\n",
    "\n",
    "if LIMIT is not None:\n",
    "    base_args += [\"--limit\", str(LIMIT)]\n",
    "\n",
    "if USE_ACCELERATE:\n",
    "    num_processes = torch.cuda.device_count()\n",
    "    command = [\n",
    "        \"accelerate\", \"launch\",\n",
    "        \"--multi_gpu\",\n",
    "        f\"--num_processes={num_processes}\",\n",
    "        \"-m\", \"lm_eval\"\n",
    "    ] + base_args\n",
    "else:\n",
    "    command = [\"lm_eval\"] + base_args\n",
    "\n",
    "print(command)\n",
    "\n",
    "subprocess.run(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Excel Sheet Template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T19:36:36.862269Z",
     "iopub.status.busy": "2025-05-06T19:36:36.861683Z",
     "iopub.status.idle": "2025-05-06T19:36:37.346135Z",
     "shell.execute_reply": "2025-05-06T19:36:37.345527Z",
     "shell.execute_reply.started": "2025-05-06T19:36:36.862243Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "columns = [\"model_uid\", \"dataset_uid\"] + LM_EVAL_UIDS\n",
    "\n",
    "model_dataset_pairs = list(product(MODEL_BASE_UIDS, DATASET_UIDS))\n",
    "\n",
    "empty_eval_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for model_uid, dataset_uid in model_dataset_pairs:\n",
    "    row = {\n",
    "        \"model_uid\": model_uid,\n",
    "        \"dataset_uid\": dataset_uid,\n",
    "    }\n",
    "    for task in LM_EVAL_UIDS:\n",
    "        row[task] = None\n",
    "    empty_eval_df.loc[len(empty_eval_df)] = row\n",
    "\n",
    "empty_eval_df.to_excel(\"empty_eval_results.xlsx\", index=False)\n",
    "\n",
    "print(\"Created empty eval_results.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "pm-pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
